import os
import time
import random
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from github import Github, GithubIntegration, Auth
from github.Repository import Repository
from github.Workflow import Workflow
import json
import yaml
import backoff
import requests
from dataclasses import dataclass
from enum import Enum
import numpy as np
from scipy import optimize

# Configure logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("GitHubAgent")

class AgentState(Enum):
    EXPLORATION = "exploration"
    EXPLOITATION = "exploitation"
    STEALTH = "stealth"
    MAINTENANCE = "maintenance"

@dataclass
class GitHubConstraintModel:
    rate_limits: Dict[str, float]  # calls per hour per endpoint
    workflow_limits: Dict[str, Any]
    pattern_analysis: Dict[str, Any]  # detected patterns of "human-like" behavior
    last_updated: datetime

class AdaptiveGitHubAgent:
    """
    An intelligent agent that learns to operate within GitHub's constraints
    through continuous adaptation and strategic planning.
    """
    
    def __init__(self, app_id: int, private_key: str, repo_name: str):
        self.app_id = app_id
        self.private_key = private_key
        self.repo_name = repo_name
        self.state = AgentState.EXPLORATION
        self.constraint_model = GitHubConstraintModel({}, {}, {}, datetime.now())
        self.operation_history = []
        self.token_pool = self.initialize_token_pool()
        self.current_token = self.select_optimal_token()
        
        # Initialize GitHub connection
        self.gh = self.initialize_github_client()
        self.repo = self.gh.get_repo(repo_name)
        
        # Learning parameters
        self.exploration_rate = 0.3  # Probability of exploring new strategies
        self.learning_rate = 0.1     # How quickly we adapt our model
        
    def initialize_token_pool(self) -> List[Dict]:
        """Initialize a pool of authentication tokens with different identities"""
        # In production, this would load from secure storage
        return [
            {"token": os.getenv("GH_TOKEN_1"), "identity": "primary", "usage_count": 0},
            {"token": os.getenv("GH_TOKEN_2"), "identity": "secondary", "usage_count": 0},
            {"token": os.getenv("GH_TOKEN_3"), "identity": "backup", "usage_count": 0}
        ]
    
    def select_optimal_token(self) -> str:
        """Select the best token based on usage patterns and constraints"""
        # Simple round-robin for now; could be enhanced with ML
        least_used = min(self.token_pool, key=lambda x: x["usage_count"])
        least_used["usage_count"] += 1
        return least_used["token"]
    
    def initialize_github_client(self):
        """Initialize GitHub client with adaptive authentication"""
        auth = Auth.Token(self.current_token)
        return Github(auth=auth)
    
    @backoff.on_exception(backoff.expo, Exception, max_tries=3)
    def adaptive_api_call(self, api_method, *args, **kwargs):
        """Make API calls with adaptive retry and rate limit handling"""
        try:
            # Check rate limits before proceeding
            self.check_rate_limits()
            
            # Execute the API call
            result = api_method(*args, **kwargs)
            
            # Update our constraint model
            self.update_constraint_model(api_method.__name__, "success")
            
            return result
            
        except Exception as e:
            self.handle_api_error(e, api_method.__name__)
            raise
    
    def check_rate_limits(self):
        """Check if we're approaching rate limits and adjust behavior"""
        rate_limit = self.gh.get_rate_limit()
        core = rate_limit.core
        
        if core.remaining / core.limit < 0.2:  # Less than 20% remaining
            self.enter_conservation_mode()
    
    def enter_conservation_mode(self):
        """Enter a conservation mode when rate limits are low"""
        self.state = AgentState.STEALTH
        logger.warning("Entering conservation mode due to rate limits")
        
        # Implement conservation strategies:
        # - Switch to different token
        # - Reduce operation frequency
        # - Use cached responses when possible
        self.current_token = self.select_optimal_token()
        self.gh = self.initialize_github_client()
    
    def handle_api_error(self, error: Exception, operation: str):
        """Analyze errors and adapt strategy accordingly"""
        error_msg = str(error).lower()
        
        # Update constraint model based on error type
        if "rate limit" in error_msg:
            self.update_constraint_model(operation, "rate_limit_exceeded")
            self.enter_conservation_mode()
        elif "automation" in error_msg or "bot" in error_msg:
            self.update_constraint_model(operation, "automation_detected")
            self.enhance_stealth_measures()
        else:
            self.update_constraint_model(operation, "other_error")
    
    def update_constraint_model(self, operation: str, outcome: str):
        """Update our understanding of GitHub's constraints"""
        now = datetime.now()
        time_since_update = (now - self.constraint_model.last_updated).total_seconds()
        
        # Simple learning: adjust based on outcomes
        if outcome == "success":
            # Gradually increase our operation frequency
            pass
        elif outcome == "rate_limit_exceeded":
            # Reduce operation frequency for this endpoint
            pass
            
        self.constraint_model.last_updated = now
        self.operation_history.append({
            "timestamp": now,
            "operation": operation,
            "outcome": outcome,
            "state": self.state.value
        })
    
    def enhance_stealth_measures(self):
        """Enhance stealth measures when automation is detected"""
        logger.info("Enhancing stealth measures")
        
        # Strategies to appear more human:
        # 1. Introduce random delays between operations
        # 2. Vary the patterns of operations
        # 3. Use different endpoints for similar operations
        # 4. Mimic human browsing patterns
        
        self.exploration_rate = min(self.exploration_rate + 0.1, 0.5)
        self.state = AgentState.STEALTH
        
        # Add random delay to operations
        time.sleep(random.uniform(1.0, 5.0))
    
    def execute_workflow(self, workflow_id: str, inputs: Optional[Dict] = None):
        """Execute a workflow with intelligent timing and input selection"""
        inputs = inputs or {}
        
        # Choose optimal time to execute based on learned patterns
        optimal_time = self.calculate_optimal_execution_time()
        current_time = datetime.now()
        
        if current_time < optimal_time:
            delay = (optimal_time - current_time).total_seconds()
            logger.info(f"Delaying workflow execution by {delay:.1f} seconds for optimal timing")
            time.sleep(delay)
        
        # Execute with adaptive retry
        return self.adaptive_api_call(
            self.repo.get_workflow(workflow_id).create_dispatch,
            ref="main",
            inputs=inputs
        )
    
    def calculate_optimal_execution_time(self) -> datetime:
        """Calculate the optimal time to execute operations"""
        # Base this on historical success patterns, time of day, etc.
        now = datetime.now()
        
        # Simple heuristic: avoid peak hours (9 AM - 5 PM UTC)
        if 9 <= now.hour <= 17:
            # Schedule for off-peak hours
            optimal_hour = random.choice([0, 1, 2, 3, 22, 23])
            optimal_time = now.replace(hour=optimal_hour, minute=0, second=0)
            
            # If we're past that time today, schedule for tomorrow
            if optimal_time < now:
                optimal_time += timedelta(days=1)
                
            return optimal_time
        
        return now  # Current time is already good
    
    def create_human_like_commit(self, message: str, changes: Dict[str, str]):
        """Create a commit that appears human-generated"""
        # Analyze successful human commit patterns
        human_patterns = self.analyze_human_commit_patterns()
        
        # Generate commit message that matches human patterns
        commit_message = self.generate_human_like_commit_message(message, human_patterns)
        
        # Introduce natural variations in commit timing and content
        self.introduce_human_like_variations()
        
        # Execute the commit
        return self.adaptive_api_call(
            self.create_commit_with_changes,
            commit_message,
            changes
        )
    
    def analyze_human_commit_patterns(self) -> Dict[str, Any]:
        """Analyze patterns in human commits to mimic them"""
        # This would analyze the repo's commit history to identify patterns
        return {
            "message_length": {"mean": 50, "std": 20},
            "commit_times": {"peak_hours": [10, 11, 14, 15]},
            "file_changes": {"mean": 3, "std": 2}
        }
    
    def generate_human_like_commit_message(self, base_message: str, patterns: Dict) -> str:
        """Generate a commit message that matches human patterns"""
        # Add human-like variations to the message
        prefixes = ["feat", "fix", "docs", "style", "refactor", "test", "chore"]
        emojis = ["âœ¨", "ğŸ›", "ğŸ“š", "ğŸ’„", "â™»ï¸", "âœ…", "ğŸ”§"]
        
        if random.random() < 0.7:  # 70% chance to use conventional commits
            prefix = random.choice(prefixes)
            emoji = random.choice(emojis)
            return f"{emoji} {prefix}: {base_message}"
        
        return base_message
    
    def introduce_human_like_variations(self):
        """Introduce human-like variations in timing and behavior"""
        # Random delay between operations
        delay = random.expovariate(1/2.0)  # Exponential distribution with mean 2 seconds
        time.sleep(min(delay, 10.0))  # Cap at 10 seconds
        
        # Occasionally introduce "mistakes" or corrections
        if random.random() < 0.05:  # 5% chance of human-like "error" behavior
            time.sleep(random.uniform(5.0, 15.0))
    
    def learn_from_environment(self):
        """Continuously learn from the environment and adapt strategies"""
        # Analyze recent operations to improve future performance
        recent_ops = [op for op in self.operation_history 
                     if (datetime.now() - op["timestamp"]).days < 7]
        
        if not recent_ops:
            return
        
        # Calculate success rates by operation type and time
        success_rates = {}
        for op in recent_ops:
            if op["operation"] not in success_rates:
                success_rates[op["operation"]] = {"success": 0, "total": 0}
            
            success_rates[op["operation"]]["total"] += 1
            if op["outcome"] == "success":
                success_rates[op["operation"]]["success"] += 1
        
        # Adjust strategies based on learning
        for op, stats in success_rates.items():
            success_rate = stats["success"] / stats["total"]
            
            if success_rate < 0.7:  # Low success rate
                logger.info(f"Operation {op} has low success rate ({success_rate:.2f}), adapting strategy")
                self.exploration_rate = min(self.exploration_rate + 0.1, 0.6)
            elif success_rate > 0.9:  # High success rate
                self.exploration_rate = max(self.exploration_rate - 0.05, 0.1)
    
    def run_continuous_learning_loop(self):
        """Run a continuous learning and adaptation loop"""
        while True:
            try:
                self.learn_from_environment()
                
                # Adjust state based on learning
                if self.exploration_rate > 0.4:
                    self.state = AgentState.EXPLORATION
                elif self.exploration_rate < 0.2:
                    self.state = AgentState.EXPLOITATION
                
                # Sleep with exponential backoff based on stability
                sleep_time = 300 / (1 + len(self.operation_history) / 100)  # 5 minutes initially, decreasing
                time.sleep(sleep_time)
                
            except Exception as e:
                logger.error(f"Error in learning loop: {e}")
                time.sleep(60)  # Wait a minute before retrying

# Example usage
if __name__ == "__main__":
    # Initialize the adaptive agent
    agent = AdaptiveGitHubAgent(
        app_id=12345,
        private_key="your-private-key",
        repo_name="your-username/your-repo"
    )
    
    # Start the learning loop in a separate thread
    import threading
    learning_thread = threading.Thread(target=agent.run_continuous_learning_loop, daemon=True)
    learning_thread.start()
    
    # Example operation
    try:
        result = agent.execute_workflow("build.yml", {"version": "1.0.0"})
        logger.info(f"Workflow executed successfully: {result}")
    except Exception as e:
        logger.error(f"Workflow execution failed: {e}")