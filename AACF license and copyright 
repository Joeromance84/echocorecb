from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
import plotly.graph_objects as go

# --- Generate Sankey diagram ---
fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=15, thickness=20,
        line=dict(color="black", width=0.5),
        label=["Sources (Africa/Global South)", "Scraper (Scrapy/Tweepy)",
               "Normalize (spaCy)", "Fact-Check (BERT)", "Database (PostgreSQL)",
               "Community Review", "AI Gatekeeper"],
        color=["#0078D7","#FF9200","#47B881","#FF4F53","#6E43CD","#CCCCCC","#2EB1ED"]
    ),
    link=dict(
        source=[0,0,1,2,3,4,5],
        target=[1,2,2,3,4,5,6],
        value=[50,20,30,25,40,35,60]
    )
)])
fig.write_image("ingestion_sankey.png")

# --- Set up PDF document ---
doc = SimpleDocTemplate("data_ingestion_spec_full.pdf", pagesize=letter)
styles = getSampleStyleSheet()
code_style = ParagraphStyle('Code', parent=styles['Normal'], fontName='Courier', fontSize=9)

elements = []
metadata = "Copyright 2025 Logan Royce Lorentz | AACF License"

def add_section(title, text, code_blocks=None):
    elements.append(Paragraph(title, styles['Heading1']))
    elements.append(Spacer(1,0.15*inch))
    elements.append(Paragraph(text, styles['Normal']))
    elements.append(Spacer(1,0.15*inch))
    if code_blocks:
        for block in code_blocks:
            elements.append(Paragraph(block, code_style))
            elements.append(Spacer(1,0.1*inch))
    elements.append(Paragraph(metadata, code_style))
    elements.append(PageBreak())

# --- Title Page ---
elements.append(Paragraph("Data Ingestion & Source Vetting Specification", styles['Title']))
elements.append(Spacer(1,0.3*inch))
elements.append(Paragraph(metadata, styles['Normal']))
elements.append(PageBreak())

# --- Overview ---
add_section(
    "1. Overview & Purpose",
    """This module prioritizes African and Global-South data sources, ensures data provenance and ethical integrity,
    embeds community consent and red line standards, and supports real-time metrics (PSI, ECI, RCF) designed for equity
    and justice. Race-based crime data and infrastructure metrics (healthcare, education, energy, internet) are directly integrated."""
)

# --- ETL Sankey ---
elements.append(Paragraph("2. ETL Ingestion Flow (Sankey Diagram)", styles['Heading1']))
elements.append(Image("ingestion_sankey.png", width=6*inch, height=3.5*inch))
elements.append(Paragraph(metadata, code_style))
elements.append(PageBreak())

# --- Canonical Sources Table ---
canon_sources = [
    ["Category", "Sources", "Priority", "Weight"],
    ["Primary", "AllAfrica, BRICS, DOJ UCR", "50%", "0.9"],
    ["Secondary", "World Bank, UN OCHA", "30%", "0.7"],
    ["Tertiary", "X posts, NGOs", "20%", "0.5"]
]
ctable = Table(canon_sources, colWidths=[1.5*inch,2.8*inch,1.2*inch,1.2*inch])
ctable.setStyle(TableStyle([
    ('BACKGROUND',(0,0),(-1,0),colors.grey),
    ('TEXTCOLOR',(0,0),(-1,0),colors.white),
    ('FONTNAME',(0,0),(-1,0),'Helvetica-Bold'),
    ('ALIGN',(0,0),(-1,-1),'CENTER'),
    ('GRID', (0,0), (-1,-1), 0.5, colors.black),
    ('BACKGROUND', (0,1), (-1,-1), colors.beige),
]))
elements.append(Paragraph("3. Canonical Source List", styles['Heading1']))
elements.append(ctable)
elements.append(Paragraph(metadata, code_style))
elements.append(PageBreak())

# --- ETL Implementation Table ---
etl_tasks = [
    ["Phase", "Tasks", "Timeline"],
    ["1","Set up Scrapy/Tweepy/PostgreSQL","Weeks 1-2"],
    ["2","Build ETL pipelines + fact-checking","Weeks 3-4"],
    ["3","Panel UI, Tupac & Bayesian heuristics","Weeks 5-6"],
    ["4","Deploy, audit, roll out to community","Weeks 7-8"]
]
etable = Table(etl_tasks, colWidths=[1.0*inch,3.0*inch,2.0*inch])
etable.setStyle(TableStyle([
    ('BACKGROUND',(0,0),(-1,0),colors.darkblue),
    ('TEXTCOLOR',(0,0),(-1,0),colors.white),
    ('GRID',(0,0),(-1,-1),0.5,colors.black),
    ('BACKGROUND', (0,1), (-1,-1), colors.beige),
]))
elements.append(Paragraph("4. Implementation Roadmap", styles['Heading1']))
elements.append(etable)
elements.append(Paragraph(metadata, code_style))
elements.append(PageBreak())

# --- Python Pseudocode: Scrapy Spider ---
add_section(
    "5. Sample Python Scrapy Spider (DOJ)",
    "",
    ["""import scrapy
class DOJSpider(scrapy.Spider):
    name = "doj_ucr"
    start_urls = ["https://ucr.fbi.gov/crime-data"]
    def parse(self, response):
        for record in response.css("table.crime-stats"):
            yield {
                "metric": "crime_rate",
                "race": record.css("td.race::text").get(),
                "value": float(record.css("td.value::text").get()),
                "metadata": {"source": "doj_ucr", "date": "2025-08-30", "copyright": "Copyright 2025 Logan Royce Lorentz"}
            }
"""]
)

# --- SQL Schema Block ---
add_section(
    "6. PostgreSQL Schema",
    "",
    ["""CREATE TABLE raw_data (
    id UUID PRIMARY KEY,
    source_id VARCHAR(50),
    content JSONB,
    scraped_at TIMESTAMP,
    confidence_score FLOAT,
    metric_category VARCHAR(50),
    copyright VARCHAR(100)
);

CREATE TABLE normalized_data (
    id UUID PRIMARY KEY,
    raw_id UUID REFERENCES raw_data(id),
    metric_type VARCHAR(20),
    value FLOAT,
    community_approved BOOLEAN,
    tupac_weight FLOAT,
    bayesian_posterior FLOAT
);
"""]
)

# --- Fact-Check + Gatekeeper Logic ---
add_section(
    "7. Automated Fact-Check & Gatekeeper Logic",
    "Automated cross-referencing, Bayesian updates for confidence, and community panel veto power safeguard data against harm. Ethical red lines prevent dehumanization.",
    ["""def gatekeeper_check(data):
    if not community_approved(data):
        return "denied: missing consent"
    if violates_ethical_redline(data):
        return "denied: harmful content"
    if tupac_weight(data) < 0.5:
        return "review: low justice alignment"
    if bayesian_posterior(data) < 0.5:
        return "review: low statistical confidence"
    return "approved"
"""]
)

# --- Infrastructure Metrics Section ---
add_section(
    "8. Infrastructure Metrics & Statistical Sciences",
    """Healthcare (beds/capita), education (graduation rates), energy (outages), internet (broadband access) are integrated. Bayesian methodologies quantify uncertainty and drive equity-focused policy scoring."""
)

# --- AACF License Appendix ---
add_section(
    "AACF License and Copyright",
    """Attribution to Logan Royce Lorentz is required; non-commercial use permitted. Data/code cannot be used for harm. Community benefit is the priority. Changes must be clearly documented."""
)

# --- Final PDF ---
doc.build(elements)
print("PDF generated: data_ingestion_spec_full.pdf")
